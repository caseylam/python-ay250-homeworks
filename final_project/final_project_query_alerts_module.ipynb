{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile query_alerts.py\n",
    "\n",
    "# FIXME: ONLY DOWNLOAD ALERT YEAR + PREVIOUS YEAR (otherwise tooons of data.)\n",
    "\n",
    "import ftplib\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.sql import select\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np \n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import time \n",
    "from urllib.request import urlopen\n",
    "import numexpr as ne\n",
    "import requests\n",
    "engine = create_engine('sqlite:///microlensing.db')\n",
    "conn = engine.connect()\n",
    "\n",
    "def get_moa_lightcurves(year):\n",
    "    # So we don't have to deal with the log10 complaining.\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    year = str(year)\n",
    "\n",
    "    url = \"http://www.massey.ac.nz/~iabond/moa/alert\" + year + \"/alert.php\"\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    links = soup.find_all('a', href=True)\n",
    "    alert_dirs = []\n",
    "    # Get a list of all the bulge microlensing alerts\n",
    "    for ii, link in enumerate(links):\n",
    "        if 'BLG' in link.text:\n",
    "            alert_dirs.append(links[ii]['href'])\n",
    "\n",
    "    for nn, alert_dir in enumerate(alert_dirs[0:10]):\n",
    "        url = \"http://www.massey.ac.nz/~iabond/moa/alert\" + year + \"/\" + alert_dir\n",
    "        response = urlopen(url)\n",
    "        html = response.read()\n",
    "        response.close()\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "        # Get the magnitude and flux offsets.\n",
    "        foo = soup.find('b').next_sibling\n",
    "        moff = foo.split('=')[1].split('-')[0].strip(' ')\n",
    "        bah = soup.find('sub').next_sibling\n",
    "        foff = bah.split('+')[1].split(')')[0].strip(' ')\n",
    "\n",
    "        # Now convert these into floats\n",
    "        m = ne.evaluate(moff)\n",
    "        f = ne.evaluate(foff)\n",
    "\n",
    "        # Now scrape the .dat file into a pandas dataframe.\n",
    "        url = \"https://www.massey.ac.nz/~iabond/moa/alert\" + year + \"/fetchtxt.php?path=moa/ephot/phot-\" + \\\n",
    "                alert_dir.strip('display.php?id=') + \".dat\"\n",
    "        bytes_data = requests.get(url).content\n",
    "        df = pd.read_csv(BytesIO(bytes_data), \n",
    "                         delim_whitespace=True, skiprows=11, skipfooter=1, header=None, engine='python', \n",
    "                         names=['hjd', 'delta_flux', 'flux_err', 'foo1', 'foo2', 'foo3', 'foo4', 'foo5'])\n",
    "\n",
    "        df['mag'] = m - 2.5*np.log10(df['delta_flux'] + f)\n",
    "        df['mag_err'] = 1.09 * df['flux_err']/(df['delta_flux'] + f)\n",
    "        df['alert_name'] = 'MB' + year[2:] + str(nn + 1).zfill(3)  # need to make sure this always works.\n",
    "        \n",
    "        df['hjd'] -= 2450000\n",
    "\n",
    "        df.dropna(axis='index', how='any', inplace=True)\n",
    "\n",
    "        cols = ['hjd', 'mag', 'mag_err', 'alert_name']\n",
    "        df[cols].to_sql(con=engine, schema=None, name=\"moa_\" + year, if_exists=\"append\", index=False)\n",
    "        \n",
    "def get_ogle_lightcurves(year):\n",
    "    year = str(year)\n",
    "\n",
    "    ftp = ftplib.FTP(\"ftp.astrouw.edu.pl\")\n",
    "    ftp.login()\n",
    "    ftp.cwd(\"ogle/ogle4/ews/\" + year + \"/\")\n",
    "\n",
    "    for nn in np.arange(start=1, stop=10, step=1):\n",
    "        ftp.cwd(\"blg-\" + str(nn).zfill(4))\n",
    "\n",
    "        flo = BytesIO()\n",
    "        ftp.retrbinary('RETR phot.dat', flo.write)\n",
    "        flo.seek(0)\n",
    "        df = pd.read_fwf(flo, header=0, names=['hjd', 'mag', 'mag_err', 'see', 'sky'])\n",
    "\n",
    "        df['alert_name'] = 'OB' + year[2:] + str(nn + 1).zfill(4) \n",
    "\n",
    "        cols = ['hjd', 'mag', 'mag_err', 'alert_name']\n",
    "        df[cols].to_sql(con=engine, schema=None, name=\"ogle_\" + year, if_exists=\"append\", index=False)\n",
    "\n",
    "        ftp.cwd(\"../\")\n",
    "        \n",
    "def get_kmtnet_lightcurves(year):\n",
    "    year = str(year)\n",
    "    \n",
    "    for nn in np.arange(start=1, stop=11, step=1):\n",
    "        # For KMTNet, get data from all the telescopes?\n",
    "        url = \"https://kmtnet.kasi.re.kr/~ulens/event/\" + year + \"/view.php?event=KMT-\" + year + \\\n",
    "                \"-BLG-\" + str(nn).zfill(4)\n",
    "        response = urlopen(url)\n",
    "        html = response.read()\n",
    "        response.close()\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "        links = soup.find_all('a', href=True)\n",
    "\n",
    "        # Only keep I-band lightcurves. \n",
    "        pysis_names = links[3].get_text(separator=',').split(',')[:-2]\n",
    "        for pysis_name in pysis_names:\n",
    "            if '_I.pysis' in pysis_name:\n",
    "                url = \"https://kmtnet.kasi.re.kr/~ulens/event/\" + year + \"/data/KB\" + \\\n",
    "                        year[2:] + str(nn).zfill(4) + \"/pysis/\" + pysis_name\n",
    "                print(url)\n",
    "                bytes_data = requests.get(url).content\n",
    "                try:\n",
    "                    df = pd.read_csv(BytesIO(bytes_data), \n",
    "                                     delim_whitespace=True, skiprows=1, header=None, \n",
    "                                     names=['hjd', 'Delta_flux', 'flux_err', 'mag', 'mag_err', 'fwhm', 'sky', 'secz'])\n",
    "\n",
    "                    df['alert_name'] = 'KB' + year[2:] + str(nn).zfill(4) \n",
    "                    df['lightcurve'] = pysis_name\n",
    "\n",
    "                    cols = ['hjd', 'mag', 'mag_err', 'lightcurve', 'alert_name']\n",
    "                    df[cols].to_sql(con=engine, schema=None, name=\"kmtnet_\" + year, if_exists=\"append\", index=False)\n",
    "                except:\n",
    "                    print('This doesn\\'t exist, skipping.'.format(nn))\n",
    "                    continue\n",
    "                    \n",
    "def get_moa_alerts(year):\n",
    "    year = str(year)\n",
    "\n",
    "    url = \"http://www.massey.ac.nz/~iabond/moa/alert2022/alert.php\"\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "    \n",
    "    # Grab columns for tE and Ibase.\n",
    "    tE = soup.find_all('td')[4::8]\n",
    "    Ibase = soup.find_all('td')[6::8]\n",
    "\n",
    "    # Convert them from strings to floats.\n",
    "    tE_list = [float(ne.evaluate(item.get_text())) for item in tE]\n",
    "    Ibase_list = [float(ne.evaluate(item.get_text())) for item in Ibase]\n",
    "\n",
    "    # Now, grab the classification column.\n",
    "    cat = soup.find_all('td')[7::8]\n",
    "    cat_list = [item.get_text() for item in cat]\n",
    "\n",
    "    # Link to the alert page.\n",
    "    alert_url = soup.find_all('td')[0::8]\n",
    "    moa_alert_url = 'http://www.massey.ac.nz/~iabond/moa/alert' + year + '/'\n",
    "    alert_url_list = [moa_alert_url + item.find_all('a', href=True)[0]['href'] for item in alert_url]\n",
    "\n",
    "    # Alert name\n",
    "    nn = len(tE_list)\n",
    "    alert_name = []\n",
    "    for ii in np.arange(nn):\n",
    "        alert_name.append('MB' + year[2:] + str(ii+1).zfill(3))\n",
    "\n",
    "    # Put it all into a dataframe.\n",
    "    df = pd.DataFrame(list(zip(alert_name, cat_list, tE_list, Ibase_list, alert_url_list)),\n",
    "                     columns =['alert_name', 'class', 'tE', 'Ibase', 'alert_url'])\n",
    "\n",
    "    df.to_sql(con=engine, schema=None, name=\"moa_alerts_\" + year, if_exists=\"replace\", index=False)\n",
    "    \n",
    "def get_ogle_alerts(year):\n",
    "    def ogle_str_to_float(item):\n",
    "        try:\n",
    "            return float(ne.evaluate(item.contents[0].replace(u'\\n', '')))\n",
    "        except:\n",
    "            return\n",
    "\n",
    "    year = str(year)\n",
    "  \n",
    "    # Get alerts using beautiful soup.\n",
    "    url = \"https://ogle.astrouw.edu.pl/ogle4/ews/\" + year + \"/ews.html\"\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "    # Grab columns for tE and Ibase.\n",
    "    tE = soup.find_all('td')[8::15] \n",
    "    Ibase = soup.find_all('td')[13::15]\n",
    "\n",
    "    # Convert them from strings to floats.\n",
    "    tE_list = [ogle_str_to_float(item) for item in tE]\n",
    "    Ibase_list = [ogle_str_to_float(item) for item in Ibase]\n",
    "\n",
    "    # Alert name and page link.\n",
    "    nn = len(tE_list)\n",
    "    alert_name = []\n",
    "    alert_url_list = []\n",
    "    ogle_alert_url = 'https://ogle.astrouw.edu.pl/ogle4/ews/'\n",
    "\n",
    "    for ii in np.arange(nn):\n",
    "        alert_name.append('OB' + year[2:] + str(ii+1).zfill(4))\n",
    "        alert_url_list.append(ogle_alert_url + str(ii+1).zfill(4) + '.html')\n",
    "\n",
    "    # Put it all into a dataframe.\n",
    "    df = pd.DataFrame(list(zip(alert_name, tE_list, Ibase_list, alert_url_list)),\n",
    "                     columns =['alert_name', 'tE', 'Ibase', 'alert_url'])\n",
    "\n",
    "    df.to_sql(con=engine, schema=None, name=\"ogle_alerts_\" + year, if_exists=\"replace\", index=False)\n",
    "    \n",
    "def get_kmtnet_alerts(year):\n",
    "    \"\"\"\n",
    "    year is an integer.\n",
    "    \"\"\"\n",
    "    def kmtnet_str_to_float(item):\n",
    "        try:\n",
    "            return float(ne.evaluate(item.get_text().replace(u'\\xa0', u'')))\n",
    "        except:\n",
    "            return\n",
    "\n",
    "    year = str(year)\n",
    "    url = \"https://kmtnet.kasi.re.kr/~ulens/event/\" + year + \"/\"\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "    if year in ['2022', '2020', '2017', '2016']:\n",
    "        tE = soup.find_all('td')[7::15][1:]\n",
    "        Ibase = soup.find_all('td')[10::15][1:]\n",
    "        cat = soup.find_all('td')[3::15][1:]\n",
    "    elif year in ['2021', '2019', '2018']:\n",
    "        tE = soup.find_all('td')[8::16][1:]\n",
    "        Ibase = soup.find_all('td')[11::16][1:]\n",
    "        cat = soup.find_all('td')[4::16][1:]\n",
    "    else:\n",
    "        raise Exception('Not a valid year')\n",
    "\n",
    "    tE_list = [kmtnet_str_to_float(item) for item in tE]\n",
    "    Ibase_list = [kmtnet_str_to_float(item) for item in Ibase]\n",
    "    cat_list = [item.get_text().replace(u'\\xa0', u'') for item in cat]\n",
    "\n",
    "    # Link to the alert page.\n",
    "    if year in ['2022', '2020', '2017', '2016']:\n",
    "        alert_url = soup.find_all('td')[0::15][1:]\n",
    "    elif year in ['2021', '2019', '2018']:\n",
    "        alert_url = soup.find_all('td')[0::16][1:]\n",
    "    else:\n",
    "        raise Exception('Not a valid year')\n",
    "    kmt_alert_url = 'https://kmtnet.kasi.re.kr/~ulens/event/' + year + '/'\n",
    "    alert_url_list = [kmt_alert_url + item.find_all('a', href=True)[0]['href'] for item in alert_url]\n",
    "\n",
    "    # Alert name\n",
    "    nn = len(tE_list)\n",
    "    alert_name = []\n",
    "    for ii in np.arange(nn):\n",
    "        alert_name.append('KB' + year[2:] + str(ii+1).zfill(4))\n",
    "\n",
    "    # Put it all into a dataframe.\n",
    "    df = pd.DataFrame(list(zip(alert_name, cat_list, tE_list, Ibase_list, alert_url_list)),\n",
    "                     columns =['alert_name', 'class', 'tE', 'Ibase', 'alert_url'])\n",
    "\n",
    "    df.to_sql(con=engine, schema=None, name=\"kmtnet_alerts_\" + year, if_exists=\"replace\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
