{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exceptional-offering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting query_alerts.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile query_alerts.py\n",
    "\n",
    "import ftplib\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.sql import select\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np \n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import time \n",
    "from urllib.request import urlopen\n",
    "import numexpr as ne\n",
    "import requests\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from itertools import repeat\n",
    "\n",
    "# Setting up database stuff with SQLAlchemy.\n",
    "engine = create_engine('sqlite:///microlensing.db')\n",
    "conn = engine.connect()\n",
    "\n",
    "def get_moa_lightcurves(year):\n",
    "    \"\"\"\n",
    "    Function that grabs MOA lightcurves from the alert pages \n",
    "    and writes them to a table in the database.\n",
    "    \n",
    "    Note that we don't care about the delta flux photometry...\n",
    "    what we really care about is the photometry in magnitudes.\n",
    "    This function takes the reported calibration values from\n",
    "    the MOA webpage, and converts the delta flux and flux error\n",
    "    measurements into magnitude and magnitude errors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        Year of the MOA alerts you want. \n",
    "        Valid choices are 2016 - 2022, inclusive.\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "    sqlite table called moa_<YYYY> in microlensing.db\n",
    "    Columns are hjd (HJD - 245000), mag, mag_err, alert_name.\n",
    "    \"\"\"\n",
    "    # The delta flux measurements sometimes yield negative fluxes\n",
    "    # after calibration. Ignore warnings so we don't have to deal\n",
    "    # with the log10 complaining during the magnitude conversion.\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Go to the MOA alerts site and scrape the page.\n",
    "    year = str(year)\n",
    "    url = \"http://www.massey.ac.nz/~iabond/moa/alert\" + year + \"/alert.php\"\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "    # Get a list of all the bulge microlensing alert directories.\n",
    "    links = soup.find_all('a', href=True)\n",
    "    alert_dirs = []\n",
    "    for ii, link in enumerate(links):\n",
    "        if 'BLG' in link.text:\n",
    "            alert_dirs.append(links[ii]['href'])\n",
    "\n",
    "    t0 = time.time() \n",
    "    # Go to the page for each bulge microlensing alert.\n",
    "    for nn, alert_dir in enumerate(alert_dirs):\n",
    "        # Scrape the page.\n",
    "        url = \"http://www.massey.ac.nz/~iabond/moa/alert\" + year + \"/\" + alert_dir\n",
    "        response = urlopen(url)\n",
    "        html = response.read()\n",
    "        response.close()\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "        # Get the magnitude and flux offsets, so we can convert\n",
    "        # from delta flux to a magnitude.\n",
    "        foo = soup.find('b').next_sibling\n",
    "        moff = foo.split('=')[1].split('-')[0].strip(' ')\n",
    "        bah = soup.find('sub').next_sibling\n",
    "        foff = bah.split('+')[1].split(')')[0].strip(' ')\n",
    "\n",
    "        # Convert those offsets from strings into floats\n",
    "        m = ne.evaluate(moff)\n",
    "        f = ne.evaluate(foff)\n",
    "\n",
    "        # Grab the .dat file containing the photometry data into a pandas dataframe.\n",
    "        url = \"https://www.massey.ac.nz/~iabond/moa/alert\" + year + \"/fetchtxt.php?path=moa/ephot/phot-\" + \\\n",
    "                alert_dir.strip('display.php?id=') + \".dat\"\n",
    "        bytes_data = requests.get(url).content\n",
    "        df = pd.read_csv(BytesIO(bytes_data), \n",
    "                         delim_whitespace=True, skiprows=11, skipfooter=1, header=None, engine='python', \n",
    "                         names=['hjd', 'delta_flux', 'flux_err', 'foo1', 'foo2', 'foo3', 'foo4', 'foo5'])\n",
    "\n",
    "        # Add columns for magnitude and magnitude error, using the conversion\n",
    "        # values we just figured out.\n",
    "        df['mag'] = m - 2.5*np.log10(df['delta_flux'] + f)\n",
    "        df['mag_err'] = 1.09 * df['flux_err']/(df['delta_flux'] + f)\n",
    "        \n",
    "        # Add a column for the alert name (of the form MBYYNNN, YY=year, NNN=alert number)\n",
    "        df['alert_name'] = 'MB' + year[2:] + str(nn + 1).zfill(3)  # need to make sure this always works.\n",
    "        \n",
    "        # Write HJD as HJD - 2450000 to match OGLE and KMTNet (less cumbersome digits)\n",
    "        df['hjd'] -= 2450000\n",
    "\n",
    "        # Get rid of all the nans which crop up during the conversion from delta flux to magnitude.\n",
    "        df.dropna(axis='index', how='any', inplace=True)\n",
    "\n",
    "        # Write out the HJD, mag, mag_err, and alert_names data into the table. \n",
    "        cols = ['hjd', 'mag', 'mag_err', 'alert_name']\n",
    "        df[cols].to_sql(con=engine, schema=None, name=\"moa_lightcurves_\" + year, if_exists=\"append\", index=False)\n",
    "    t1 = time.time() \n",
    "    \n",
    "    print('Took {0:.2f} seconds'.format(t1-t0))\n",
    "    \n",
    "def get_ogle_lightcurves(year):\n",
    "    \"\"\"\n",
    "    Function that grabs OGLE lightcurves from the alert website \n",
    "    and writes them to a table in the database.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        Year of the MOA alerts you want. \n",
    "        Valid choices are 2011 - 2029, inclusive.\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "    sqlite table called ogle_<YYYY> in microlensing.db\n",
    "    Columns are hjd (HJD - 245000), mag, mag_err, alert_name.\n",
    "    \"\"\"\n",
    "    # Go to the OGLE alert site and get the data with FTP.\n",
    "    year = str(year)\n",
    "    ftp = ftplib.FTP(\"ftp.astrouw.edu.pl\")\n",
    "    ftp.login()\n",
    "    ftp.cwd(\"ogle/ogle4/ews/\" + year + \"/\")\n",
    "    \n",
    "    # Figure out how many objects there are by counting the number of .tar.gz files \n",
    "    # in the directory (each .tar.gz file corresponds to an alert)\n",
    "    nobj = sum('.tar.gz' in x for x in ftp.nlst())\n",
    " \n",
    "    t0 = time.time() \n",
    "    for nn in np.arange(start=1, stop=nobj+1, step=1):\n",
    "        # Grab the photometry for each alert.\n",
    "        ftp.cwd(\"blg-\" + str(nn).zfill(4))\n",
    "        \n",
    "        flo = BytesIO()\n",
    "        ftp.retrbinary('RETR phot.dat', flo.write)\n",
    "        flo.seek(0)\n",
    "        df = pd.read_fwf(flo, header=0, \n",
    "                         names=['hjd', 'mag', 'mag_err', 'see', 'sky'], \n",
    "                         widths=[14, 7, 6, 5, 8])\n",
    "\n",
    "        # Add a column for the alert name (of the form OBYYNNNN, YY=year, NNN=alert number)\n",
    "        df['alert_name'] = 'OB' + year[2:] + str(nn + 1).zfill(4) \n",
    "\n",
    "        # Write out the HJD, mag, mag_err, and alert_names data into the table. \n",
    "        cols = ['hjd', 'mag', 'mag_err', 'alert_name']\n",
    "        df[cols].to_sql(con=engine, schema=None, name=\"ogle_lightcurves_\" + year, if_exists=\"append\", index=False)\n",
    "\n",
    "        ftp.cwd(\"../\")\n",
    "    t1 = time.time() \n",
    "    ftp.close()\n",
    "    \n",
    "    print('Took {0:.2f} seconds'.format(t1-t0))\n",
    "    \n",
    "def get_kmtnet_lightcurves(year):\n",
    "    \"\"\"\n",
    "    Function that grabs KMTNet lightcurves from the alert pages \n",
    "    and writes them to a table in the database.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        Year of the KMTNet alerts you want. \n",
    "        Valid choices are 2016 - 2022, inclusive.\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "    sqlite table called kmtnet_<YYYY> in microlensing.db\n",
    "    Columns are hjd (HJD - 245000), mag, mag_err, alert_name, and lightcurve (the pysis name).\n",
    "    \"\"\"\n",
    "    # Figure out how many objects there are by counting how many columns\n",
    "    # there are on the alert page.\n",
    "    year = str(year)\n",
    "    url = \"https://kmtnet.kasi.re.kr/~ulens/event/\" + year + \"/\"\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    nobj = len(soup.find_all('td')[0::15][1:])\n",
    "    \n",
    "    t0 = time.time()\n",
    "    # Go to the KMTNet alerts site and scrape the page for each alert.\n",
    "    for nn in np.arange(start=1, stop=nobj+1, step=1):\n",
    "        url = \"https://kmtnet.kasi.re.kr/~ulens/event/\" + year + \\\n",
    "                \"/view.php?event=KMT-\" + year + \"-BLG-\" + str(nn).zfill(4)\n",
    "        response = urlopen(url)\n",
    "        html = response.read()\n",
    "        response.close()\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "        # Get the names of all the different lightcurve files (pysis names).\n",
    "        links = soup.find_all('a', href=True)\n",
    "        pysis_names = links[3].get_text(separator=',').split(',')[:-2]\n",
    "        \n",
    "        # Note, we are only keeping I-band lightcurves (V-band ones are not useful). \n",
    "        for pysis_name in pysis_names:\n",
    "            if '_I.pysis' in pysis_name:\n",
    "                # Grab the photometry for each alert's I-band lightcurve data into a pands dataframe.\n",
    "                url = \"https://kmtnet.kasi.re.kr/~ulens/event/\" + year + \"/data/KB\" + \\\n",
    "                        year[2:] + str(nn).zfill(4) + \"/pysis/\" + pysis_name\n",
    "                bytes_data = requests.get(url).content\n",
    "                df = pd.read_csv(BytesIO(bytes_data), \n",
    "                                 delim_whitespace=True, skiprows=1, header=None, \n",
    "                                 names=['hjd', 'Delta_flux', 'flux_err', 'mag', \n",
    "                                        'mag_err', 'fwhm', 'sky', 'secz'])\n",
    "\n",
    "                # Add columns for the alert name (of the form KBYYNNNN, YY=year, NNNN=alert number)\n",
    "                # and the name of the lightcurve's pysis file.\n",
    "                df['alert_name'] = 'KB' + year[2:] + str(nn).zfill(4) \n",
    "                df['lightcurve'] = pysis_name\n",
    "\n",
    "                # Write out the HJD, mag, mag_err, lightcurve, and alert_name data into the table.\n",
    "                cols = ['hjd', 'mag', 'mag_err', 'lightcurve', 'alert_name']\n",
    "                df[cols].to_sql(con=engine, schema=None, name=\"kmtnet_lightcurves_\" + year, \n",
    "                                if_exists=\"append\", index=False)\n",
    "    t1 = time.time()             \n",
    "    \n",
    "    print('Took {0:.2f} seconds'.format(t1-t0))\n",
    "\n",
    "def get_moa_params(alert_dir, year, nn):  \n",
    "    \"\"\"\n",
    "    Get all the different MOA alert parameters (along with their uncertainties)\n",
    "    from the individual web pages. The uncertainties are not listed on the \n",
    "    front summary page unfortunately.\n",
    "    \n",
    "    The \"_e\" values are the uncertainties.\n",
    "    \"\"\"\n",
    "    # Add a column for the alert name (of the form MBYYNNN, YY=year, NNN=alert number)\n",
    "    alert_name = 'MB' + year[2:] + str(nn + 1).zfill(3)  # need to make sure this always works.\n",
    "    \n",
    "    # Go to the alert page and scrape the data.\n",
    "    url = \"http://www.massey.ac.nz/~iabond/moa/alert\" + year + \"/\" + alert_dir\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "    # Parse the scraped data.\n",
    "    meta = soup.find('div', id=\"metadata\").text\n",
    "    RA = meta.split('RA:')[1].split('Dec:')[0]\n",
    "    Dec = meta.split('RA:')[1].split('Dec:')[1].split('Current')[0]\n",
    "\n",
    "    tmax_str = soup.find('div', id=\"lastphot\").text.split('<td>=<td align=right>')[1]\n",
    "    tmax = moa_str_to_float(tmax_str.split()[1])\n",
    "    tmax_e = moa_str_to_float(tmax_str.split('<td>')[2].split()[0])\n",
    "\n",
    "    tE_str = soup.find('div', id=\"lastphot\").text.split('<td>=<td align=right>')[2]\n",
    "    tE = moa_str_to_float(tE_str.split()[0])\n",
    "    tE_e = moa_str_to_float(tE_str.split('<td>')[2].split()[0])\n",
    "\n",
    "    u0_str = soup.find('div', id=\"lastphot\").text.split('<td>=<td align=right>')[3]\n",
    "    u0 = moa_str_to_float(u0_str.split()[0])\n",
    "    u0_e = moa_str_to_float(u0_str.split('<td>')[2].split()[0].split('<')[0])\n",
    "\n",
    "    Ibase_str = soup.find('div', id=\"lastphot\").text.split('<td>=<td align=right>')[4]\n",
    "    Ibase = moa_str_to_float(Ibase_str.split()[0])\n",
    "    Ibase_e = moa_str_to_float(Ibase_str.split('<td>')[2].split()[0].split('<')[0])\n",
    "\n",
    "    assessment = soup.find('div', id=\"metadata\").find_all('td', align='right')[4].text\n",
    "    \n",
    "    return alert_name, RA, Dec, tmax, tmax_e, tE, tE_e, \\\n",
    "            u0, u0_e, Ibase, Ibase_e, assessment, url\n",
    "    \n",
    "def get_moa_alerts(year):\n",
    "    \"\"\"\n",
    "    Function that grabs all the different MOA alert parameters \n",
    "    (along with their uncertainties) for any given alert year, \n",
    "    and write them into a database.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        Year of the OGLE alerts you want.\n",
    "        Valid choices are 2001 - 2019, inclusive.\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "    sqlite table called moa_alerts_<YYYY> in microlensing.db\n",
    "    \"\"\"    \n",
    "    # Go to the MOA alerts site and scrape the page.\n",
    "    year = str(year)\n",
    "    url = \"http://www.massey.ac.nz/~iabond/moa/alert\" + year + \"/alert.php\"\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "    # Get a list of all the bulge microlensing alert directories.\n",
    "    links = soup.find_all('a', href=True)\n",
    "    alert_dirs = []\n",
    "    for ii, link in enumerate(links):\n",
    "        if 'BLG' in link.text:\n",
    "            alert_dirs.append(links[ii]['href'])\n",
    "        \n",
    "    # Figure out how many alerts there are in total\n",
    "    npages = len(alert_dirs)\n",
    "\n",
    "    # Go to the page for each bulge microlensing alert and scrape the parameters.\n",
    "    # This process is parallelized using Pool (it's really slow to have to loop\n",
    "    # over all pages, and this is an embarassingly parallel process.)\n",
    "    _t0 = time.time()     \n",
    "    num_workers = mp.cpu_count()  \n",
    "    pool = mp.Pool(processes=num_workers)\n",
    "    parallel_results = pool.starmap(get_moa_params, \n",
    "                                    zip(alert_dirs, repeat(year), range(npages)))\n",
    "    _t1 = time.time()\n",
    "    \n",
    "    # Put it all into a dataframe and write out to the database.\n",
    "    df = pd.DataFrame(parallel_results,\n",
    "                     columns = ['alert_name', 'RA', 'Dec', 'tmax', 'tmax_e', 'tE', 'tE_e', \n",
    "                                'u0', 'u0_e', 'Ibase', 'Ibase_e', 'assessment', 'alert_url'])\n",
    "\n",
    "    # Write HJD as HJD - 2450000 (less cumbersome digits)\n",
    "    df['tmax'] -= 2450000\n",
    "    \n",
    "    df.to_sql(con=engine, schema=None, name=\"moa_alerts_\" + year, if_exists=\"replace\", index=False)\n",
    "    \n",
    "    _t1 = time.time()\n",
    "    print('Took {0:.2f} seconds'.format(_t1-_t0))\n",
    "\n",
    "def get_ogle_params(year, nn):  \n",
    "    \"\"\"\n",
    "    Get all the different OGLE alert parameters (along with their uncertainties)\n",
    "    from the individual web pages. The uncertainties are not listed on the \n",
    "    front summary page or lenses.par file in the data download unfortunately.\n",
    "    \n",
    "    The \"_e\" values are the uncertainties.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add a column for the alert name (of the form OBYYNNNN, YY=year, NNN=alert number)\n",
    "    alert_name = 'OB' + year[2:] + str(nn + 1).zfill(4) \n",
    "    \n",
    "    # Go to the alert page and scrape the data.\n",
    "    url = \"https://ogle.astrouw.edu.pl/ogle4/ews/\" + year + \\\n",
    "            \"/blg-\" + str(nn+1).zfill(4) + \".html\"\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    header_list = soup.find_all('table')[1].find('td').text.split()\n",
    "    param_list = soup.find_all('table')[2].find('td').text.split()\n",
    "\n",
    "    # Parse the scraped data.\n",
    "    RA = header_list[7]\n",
    "    Dec = header_list[10]\n",
    "    Tmax = ogle_str_to_float(param_list, 1)\n",
    "    Tmax_e = ogle_str_to_float(param_list, 3)\n",
    "    tau =  ogle_str_to_float(param_list, 7)\n",
    "    tau_e =  ogle_str_to_float(param_list, 9)\n",
    "    Umin =  ogle_str_to_float(param_list, 11)\n",
    "    Umin_e =  ogle_str_to_float(param_list, 13)\n",
    "    Amax =  ogle_str_to_float(param_list, 15)\n",
    "    Amax_e =  ogle_str_to_float(param_list, 17)\n",
    "    Dmag =  ogle_str_to_float(param_list, 19)\n",
    "    Dmag_e =  ogle_str_to_float(param_list, 21)\n",
    "    fbl =  ogle_str_to_float(param_list, 23)\n",
    "    fbl_e =  ogle_str_to_float(param_list, 25)\n",
    "    Ibl =  ogle_str_to_float(param_list, 27)\n",
    "    Ibl_e =  ogle_str_to_float(param_list, 29)\n",
    "    I0 = ogle_str_to_float(param_list, 31)\n",
    "    I0_e =  ogle_str_to_float(param_list, 33)\n",
    "\n",
    "    return alert_name, RA, Dec, Tmax, Tmax_e, tau, tau_e, Umin, Umin_e, \\\n",
    "            Amax, Amax_e, Dmag, Dmag_e, fbl, fbl_e, Ibl, Ibl_e, I0, I0_e, url\n",
    "    \n",
    "def ogle_str_to_float(list_in, idx):\n",
    "    \"\"\"\n",
    "    Little helper function to turn strings into floats.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return float(ne.evaluate(list_in[idx]))\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def moa_str_to_float(str_in):\n",
    "    \"\"\"\n",
    "    Little helper function to turn strings into floats.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return float(ne.evaluate(str_in))\n",
    "    except:\n",
    "        return np.nan\n",
    "        \n",
    "def get_ogle_alerts(year):\n",
    "    \"\"\"\n",
    "    Function that grabs all the different OGLE alert parameters \n",
    "    (along with their uncertainties) for any given alert year, \n",
    "    and write them into a database.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        Year of the OGLE alerts you want.\n",
    "        Valid choices are 2001 - 2019, inclusive.\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "    sqlite table called ogle_alerts_<YYYY> in microlensing.db\n",
    "    \"\"\"\n",
    "    # Go to the OGLE alerts site and scrape the page.\n",
    "    year = str(year)\n",
    "    url = \"https://ogle.astrouw.edu.pl/ogle4/ews/\" + year + \"/ews.html\"\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "    # Figure out how many alert pages there are.\n",
    "    npages = len(soup.find_all('td')[0::15])\n",
    "    \n",
    "    # Grab all the parameters from the OGLE pages.\n",
    "    # Use pool to parallelize (it is very slow otherwise,\n",
    "    # since we have to loop through each page individually.)\n",
    "    _t0 = time.time()     \n",
    "    num_workers = mp.cpu_count()  \n",
    "    pool = mp.Pool(processes=num_workers)\n",
    "    parallel_results = pool.starmap(get_ogle_params, zip(repeat(year), range(npages)))\n",
    "    _t1 = time.time() \n",
    "\n",
    "    # Put it all into a dataframe and write out to the database.\n",
    "    df = pd.DataFrame(parallel_results,\n",
    "                     columns =['alert_name', 'RA', 'Dec', 'Tmax', 'Tmax_e', 'tau', 'tau_e', \n",
    "                               'Umin', 'Umin_e', 'Amax', 'Amax_e', 'Dmag', 'Dmag_e', \n",
    "                               'fbl', 'fbl_e', 'Ibl', 'Ibl_e', 'I0', 'I0_e', 'alert_url'])\n",
    "\n",
    "    # Write HJD as HJD - 2450000 (less cumbersome digits)\n",
    "    df['Tmax'] -= 2450000\n",
    "\n",
    "    df.to_sql(con=engine, schema=None, name=\"ogle_alerts_\" + year, if_exists=\"replace\", index=False)\n",
    "    \n",
    "    print('Took {0:.2f} seconds'.format(_t1-_t0))\n",
    "    \n",
    "def get_kmtnet_alerts(year):\n",
    "    \"\"\"\n",
    "    Function that grabs KMTNet alerts and writes the fit\n",
    "    tE and Ibase parameters, as well as each alert's \n",
    "    classification, to a table in the database.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        Year of the KMTNet alerts you want.\n",
    "        Valid choices are 2016 - 2022, inclusive.\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "    sqlite table called kmtnet_alerts_<YYYY> in microlensing.db\n",
    "    Columns are alert_name, class, tE, Ibase, alert_url.\n",
    "    \"\"\"\n",
    "    def kmtnet_str_to_float(item):\n",
    "        try:\n",
    "            return float(ne.evaluate(item.get_text().replace(u'\\xa0', u'')))\n",
    "        except:\n",
    "            return\n",
    "        \n",
    "    # Go to the KMTNet alerts site and scrape the page.\n",
    "    year = str(year)\n",
    "    url = \"https://kmtnet.kasi.re.kr/~ulens/event/\" + year + \"/\"\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "    # For some annoying reason, the KMTNet alerts system changes\n",
    "    # across years randomly. Some years they report a single\n",
    "    # classification for alerts, other years there are two \n",
    "    # classifications (\"EF\" and \"AL\", I don't know what it means).\n",
    "    # For years where there are two classifications, I've picked \n",
    "    # AL classification arbitrarily.\n",
    "    if year in ['2022', '2020', '2017', '2016']:\n",
    "        class_ = soup.find_all('td')[3::15][1:]\n",
    "        RA = soup.find_all('td')[4::15][1:]\n",
    "        Dec = soup.find_all('td')[5::15][1:]\n",
    "        t_0 = soup.find_all('td')[6::15][1:]\n",
    "        t_E = soup.find_all('td')[7::15][1:]\n",
    "        u_0 = soup.find_all('td')[8::15][1:]\n",
    "        Isource = soup.find_all('td')[9::15][1:]\n",
    "        Ibase = soup.find_all('td')[10::15][1:]\n",
    "        Icat \n",
    "    elif year in ['2021', '2019', '2018']:\n",
    "        tE = soup.find_all('td')[8::16][1:]\n",
    "        Ibase = soup.find_all('td')[11::16][1:]\n",
    "        cat = soup.find_all('td')[4::16][1:]\n",
    "    else:\n",
    "        raise Exception('Not a valid year')\n",
    "\n",
    "    # Process output to get strings/floats as appropriate.\n",
    "    tE_list = [kmtnet_str_to_float(item) for item in tE]\n",
    "    Ibase_list = [kmtnet_str_to_float(item) for item in Ibase]\n",
    "    class_list = [item.get_text().replace(u'\\xa0', u'') for item in class_]\n",
    "\n",
    "    # Get link to the alert page.\n",
    "    if year in ['2022', '2020', '2017', '2016']:\n",
    "        alert_url = soup.find_all('td')[0::15][1:]\n",
    "    elif year in ['2021', '2019', '2018']:\n",
    "        alert_url = soup.find_all('td')[0::16][1:]\n",
    "    else:\n",
    "        raise Exception('Not a valid year')\n",
    "    kmt_alert_url = 'https://kmtnet.kasi.re.kr/~ulens/event/' + year + '/'\n",
    "    alert_url_list = [kmt_alert_url + item.find_all('a', href=True)[0]['href'] for item in alert_url]\n",
    "\n",
    "    # Get alert name\n",
    "    nn = len(tE_list)\n",
    "    alert_name = []\n",
    "    for ii in np.arange(nn):\n",
    "        alert_name.append('KB' + year[2:] + str(ii+1).zfill(4))\n",
    "\n",
    "    # Put it all into a dataframe and write out to the database.\n",
    "    df = pd.DataFrame(list(zip(alert_name, cat_list, tE_list, Ibase_list, alert_url_list)),\n",
    "                     columns =['alert_name', 'class', 'tE', 'Ibase', 'alert_url'])\n",
    "\n",
    "    df.to_sql(con=engine, schema=None, name=\"kmtnet_alerts_\" + year, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mighty-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftplib\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.sql import select\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np \n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import time \n",
    "from urllib.request import urlopen\n",
    "import numexpr as ne\n",
    "import requests\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from itertools import repeat\n",
    "\n",
    "url = \"https://kmtnet.kasi.re.kr/~ulens/event/2016/listpage.dat\"\n",
    "response = urlopen(url)\n",
    "html = response.read()\n",
    "response.close()\n",
    "soup = BeautifulSoup(html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "listed-disposal",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 15 fields in line 6, saw 16\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fc6b8e95abe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mdelim_whitespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_default_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0;31m# skiprows=11, skipfooter=1,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                  header=None) \n\u001b[0m\u001b[1;32m      7\u001b[0m                  \u001b[0;31m#engine='python',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                   \u001b[0;31m#       names=['hjd', 'delta_flux', 'flux_err', 'foo1', 'foo2', 'foo3', 'foo4', 'foo5'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/astroconda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/astroconda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/astroconda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/astroconda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2059\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 15 fields in line 6, saw 16\n"
     ]
    }
   ],
   "source": [
    "url = \"https://kmtnet.kasi.re.kr/~ulens/event/2016/listpage.dat\"\n",
    "bytes_data = requests.get(url).content\n",
    "df = pd.read_csv(BytesIO(bytes_data), \n",
    "                         delim_whitespace=True, \n",
    "                 # skiprows=11, skipfooter=1, \n",
    "                 header=None) \n",
    "                 #engine='python', \n",
    "                  #       names=['hjd', 'delta_flux', 'flux_err', 'foo1', 'foo2', 'foo3', 'foo4', 'foo5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "psychological-hindu",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KMT-2016-BLG-0001 SAO02K0103.022072 K2 1 17:54:14.09 -28:40:58.91 7549.19296   39.10  0.704  19.05  19.04  19.04 2   2.14 OB160785</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KMT-2016-BLG-0002 BLG02K0105.005671 K2 1 17:54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KMT-2016-BLG-0003 SAO02K0105.007856 K2 1 17:54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KMT-2016-BLG-0004 BLG02K0203.015135 K2 1 17:53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KMT-2016-BLG-0005 SSO02K0303.015757 K2 1 17:53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KMT-2016-BLG-0006 SAO02K0406.017651 K2 1 17:52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582</th>\n",
       "      <td>KMT-2016-BLG-2584 BLG42M0706.041029  - 3 17:55...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>KMT-2016-BLG-2585 BLG42M0804.027561  - 3 17:54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2584</th>\n",
       "      <td>KMT-2016-BLG-2586 SAO42M0805.086923  - 3 17:54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2585</th>\n",
       "      <td>KMT-2016-BLG-2587 BLG42T0405.080278  - 3 17:52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>KMT-2016-BLG-2588 BLG43M0704.093906  - 3 18:02...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2587 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     KMT-2016-BLG-0001 SAO02K0103.022072 K2 1 17:54:14.09 -28:40:58.91 7549.19296   39.10  0.704  19.05  19.04  19.04 2   2.14 OB160785\n",
       "0     KMT-2016-BLG-0002 BLG02K0105.005671 K2 1 17:54...                                                                                \n",
       "1     KMT-2016-BLG-0003 SAO02K0105.007856 K2 1 17:54...                                                                                \n",
       "2     KMT-2016-BLG-0004 BLG02K0203.015135 K2 1 17:53...                                                                                \n",
       "3     KMT-2016-BLG-0005 SSO02K0303.015757 K2 1 17:53...                                                                                \n",
       "4     KMT-2016-BLG-0006 SAO02K0406.017651 K2 1 17:52...                                                                                \n",
       "...                                                 ...                                                                                \n",
       "2582  KMT-2016-BLG-2584 BLG42M0706.041029  - 3 17:55...                                                                                \n",
       "2583  KMT-2016-BLG-2585 BLG42M0804.027561  - 3 17:54...                                                                                \n",
       "2584  KMT-2016-BLG-2586 SAO42M0805.086923  - 3 17:54...                                                                                \n",
       "2585  KMT-2016-BLG-2587 BLG42T0405.080278  - 3 17:52...                                                                                \n",
       "2586  KMT-2016-BLG-2588 BLG43M0704.093906  - 3 18:02...                                                                                \n",
       "\n",
       "[2587 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-cardiff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
