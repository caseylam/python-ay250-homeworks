{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5335617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftplib\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.sql import select\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np \n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import time \n",
    "from urllib.request import urlopen\n",
    "import numexpr as ne\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd869df",
   "metadata": {},
   "source": [
    "This notebook outlines how to dump the lightcurves and alerts into databases. Let's use SQLAlchemy Core.\n",
    "\n",
    "First, let's put the lightcurves into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce343613",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///foo.db')\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e68f35",
   "metadata": {},
   "source": [
    "# MOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "972ed940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to write table: 22 sec for 10 alerts\n"
     ]
    }
   ],
   "source": [
    "# So we don't have to deal with the log10 complaining.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "url = \"http://www.massey.ac.nz/~iabond/moa/alert2022/alert.php\"\n",
    "response = urlopen(url)\n",
    "html = response.read()\n",
    "response.close()\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "links = soup.find_all('a', href=True)\n",
    "alert_dirs = []\n",
    "# Get a list of all the bulge microlensing alerts\n",
    "for ii, link in enumerate(links):\n",
    "    if 'BLG' in link.text:\n",
    "        alert_dirs.append(links[ii]['href'])\n",
    "        \n",
    "counter = 0\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for nn, alert_dir in enumerate(alert_dirs[0:10]):\n",
    "    url = \"http://www.massey.ac.nz/~iabond/moa/alert2022/\" + alert_dir\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "    # Get the magnitude and flux offsets.\n",
    "    foo = soup.find('b').next_sibling\n",
    "    moff = foo.split('=')[1].split('-')[0].strip(' ')\n",
    "    bah = soup.find('sub').next_sibling\n",
    "    foff = bah.split('+')[1].split(')')[0].strip(' ')\n",
    "\n",
    "    # Now convert these into floats\n",
    "    m = ne.evaluate(moff)\n",
    "    f = ne.evaluate(foff)\n",
    "\n",
    "    # Now scrape the .dat file into a pandas dataframe.\n",
    "    url = \"https://www.massey.ac.nz/~iabond/moa/alert2022/fetchtxt.php?path=moa/ephot/phot-\" + \\\n",
    "            alert_dir.strip('display.php?id=') + \".dat\"\n",
    "    bytes_data = requests.get(url).content\n",
    "    df = pd.read_csv(BytesIO(bytes_data), \n",
    "                     delim_whitespace=True, skiprows=11, skipfooter=1, header=None, engine='python', \n",
    "                     names=['hjd', 'delta_flux', 'flux_err', 'foo1', 'foo2', 'foo3', 'foo4', 'foo5'])\n",
    "\n",
    "    df['mag'] = m - 2.5*np.log10(df['delta_flux'] + f)\n",
    "    df['mag_err'] = 1.09 * df['flux_err']/(df['delta_flux'] + f)\n",
    "    df['alert_name'] = 'MB22' + str(nn + 1).zfill(3)  # need to make sure this always works.\n",
    "\n",
    "    df.dropna(axis='index', how='any', inplace=True)\n",
    "\n",
    "    cols = ['hjd', 'mag', 'mag_err', 'alert_name']\n",
    "    df[cols].to_sql(con=engine, schema=None, name=\"moa\", if_exists=\"append\", index=False)\n",
    "    counter += 1\n",
    "t1 = time.time()\n",
    "\n",
    "print('Time to write table: {0:.0f} sec for {1} alerts'.format(t1 - t0, counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c328ce0",
   "metadata": {},
   "source": [
    "# OGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86cda875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to write table: 18 sec for 9 alerts\n"
     ]
    }
   ],
   "source": [
    "ftp = ftplib.FTP(\"ftp.astrouw.edu.pl\")\n",
    "ftp.login()\n",
    "ftp.cwd(\"ogle/ogle4/ews/2019/\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for nn in np.arange(start=1, stop=10, step=1):\n",
    "    ftp.cwd(\"blg-\" + str(nn).zfill(4))\n",
    "    \n",
    "    flo = BytesIO()\n",
    "    ftp.retrbinary('RETR phot.dat', flo.write)\n",
    "    flo.seek(0)\n",
    "    df = pd.read_fwf(flo, header=0, names=['hjd', 'mag', 'mag_err', 'see', 'sky'])\n",
    "    \n",
    "    df['alert_name'] = 'OB19' + str(nn + 1).zfill(4) \n",
    "\n",
    "    cols = ['hjd', 'mag', 'mag_err', 'alert_name']\n",
    "    df[cols].to_sql(con=engine, schema=None, name=\"ogle\", if_exists=\"append\", index=False)\n",
    "    \n",
    "    ftp.cwd(\"../\")\n",
    "    \n",
    "    counter += 1\n",
    "t1 = time.time()\n",
    "\n",
    "print('Time to write table: {0:.0f} sec for {1} alerts'.format(t1 - t0, counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef9e8e",
   "metadata": {},
   "source": [
    "# KMTNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9192a462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to write table: 79 sec for 109 files (10 alerts)\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for nn in np.arange(start=1, stop=11, step=1):\n",
    "    # For KMTNet, get data from all the telescopes?\n",
    "    url = \"https://kmtnet.kasi.re.kr/~ulens/event/2022/view.php?event=KMT-2022-BLG-\" + str(nn).zfill(4)\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    \n",
    "    links = soup.find_all('a', href=True)\n",
    "\n",
    "    # Only keep I-band lightcurves. \n",
    "    pysis_names = links[3].get_text(separator=',').split(',')[:-2]\n",
    "    \n",
    "    for pysis_name in pysis_names:\n",
    "        url = \"https://kmtnet.kasi.re.kr/~ulens/event/2022/data/KB22\" + str(nn).zfill(4) + \"/pysis/\" + pysis_name\n",
    "        bytes_data = requests.get(url).content\n",
    "        try:\n",
    "            df = pd.read_csv(BytesIO(bytes_data), \n",
    "                             delim_whitespace=True, skiprows=1, header=None, \n",
    "                             names=['hjd', 'Delta_flux', 'flux_err', 'mag', 'mag_err', 'fwhm', 'sky', 'secz'])\n",
    "\n",
    "            df['alert_name'] = 'KB22' + str(nn + 1).zfill(4) \n",
    "            df['lightcurve'] = pysis_name\n",
    "\n",
    "            cols = ['hjd', 'mag', 'mag_err', 'lightcurve', 'alert_name']\n",
    "            df[cols].to_sql(con=engine, schema=None, name=\"kmtnet\", if_exists=\"append\", index=False)\n",
    "            counter += 1\n",
    "        except:\n",
    "            print('This doesn\\'t exist, skipping.'.format(nn))\n",
    "            continue\n",
    "t1 = time.time()\n",
    "\n",
    "print('Time to write table: {0:.0f} sec for {1} files ({2} alerts)'.format(t1 - t0, counter, nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e2a14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to write table: 45 sec for 56 files (10 alerts)\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for nn in np.arange(start=1, stop=11, step=1):\n",
    "    # For KMTNet, get data from all the telescopes?\n",
    "    url = \"https://kmtnet.kasi.re.kr/~ulens/event/2022/view.php?event=KMT-2022-BLG-\" + str(nn).zfill(4)\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    \n",
    "    links = soup.find_all('a', href=True)\n",
    "\n",
    "    # Only keep I-band lightcurves. \n",
    "    # FIXME: Could probably trim this further down by only taking the one with the most data?\n",
    "    # But not sure if checking is more expensive than just writing it.\n",
    "    pysis_names = links[3].get_text(separator=',').split(',')[:-2]\n",
    "    \n",
    "    for pysis_name in pysis_names:\n",
    "        if '_I.pysis' in pysis_name:\n",
    "            url = \"https://kmtnet.kasi.re.kr/~ulens/event/2022/data/KB22\" + str(nn+1).zfill(4) + \"/pysis/\" + pysis_name\n",
    "            bytes_data = requests.get(url).content\n",
    "            try:\n",
    "                df = pd.read_csv(BytesIO(bytes_data), \n",
    "                                 delim_whitespace=True, skiprows=1, header=None, \n",
    "                                 names=['hjd', 'Delta_flux', 'flux_err', 'mag', 'mag_err', 'fwhm', 'sky', 'secz'])\n",
    "\n",
    "                df['alert_name'] = 'KB22' + str(nn + 1).zfill(4) \n",
    "                df['lightcurve'] = pysis_name\n",
    "                \n",
    "                cols = ['hjd', 'mag', 'mag_err', 'lightcurve', 'alert_name']\n",
    "                df[cols].to_sql(con=engine, schema=None, name=\"kmtnet\", if_exists=\"append\", index=False)\n",
    "                counter += 1\n",
    "            except:\n",
    "                print('This doesn\\'t exist, skipping.'.format(nn))\n",
    "                continue\n",
    "t1 = time.time()\n",
    "\n",
    "print('Time to write table: {0:.0f} sec for {1} files ({2} alerts)'.format(t1 - t0, counter, nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b0043a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = text('DROP TABLE kmtnet')\n",
    "# result = conn.execute(s)\n",
    "\n",
    "# s = text('DROP TABLE ogle')\n",
    "# result = conn.execute(s)\n",
    "\n",
    "# s = text('DROP TABLE moa')\n",
    "# result = conn.execute(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39f4d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First way to query the table.\n",
    "# result = engine.execute(\"SELECT HJD FROM kmtnet\").fetchall()\n",
    "\n",
    "# Second way to query the table.\n",
    "# s = text('SELECT * FROM kmtnet')\n",
    "# result = conn.execute(s)\n",
    "# result.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5799d25e",
   "metadata": {},
   "source": [
    "Next, we'll put the alerts into the database. \n",
    "\n",
    "Do we want to post all alert values, or just the subset I have here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b6e158",
   "metadata": {},
   "source": [
    "# MOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbbc8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.massey.ac.nz/~iabond/moa/alert2022/alert.php\"\n",
    "response = urlopen(url)\n",
    "html = response.read()\n",
    "response.close()\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "# Grab columns for tE and Ibase.\n",
    "tE = soup.find_all('td')[4::8]\n",
    "Ibase = soup.find_all('td')[6::8]\n",
    "\n",
    "# Convert them from strings to floats.\n",
    "tE_list = [float(ne.evaluate(item.get_text())) for item in tE]\n",
    "Ibase_list = [float(ne.evaluate(item.get_text())) for item in Ibase]\n",
    "\n",
    "# Now, grab the classification column.\n",
    "cat = soup.find_all('td')[7::8]\n",
    "cat_list = [item.get_text() for item in cat]\n",
    "\n",
    "# Link to the alert page.\n",
    "alert_url = soup.find_all('td')[0::8]\n",
    "moa_alert_url = 'http://www.massey.ac.nz/~iabond/moa/alert2022/'\n",
    "alert_url_list = [moa_alert_url + item.find_all('a', href=True)[0]['href'] for item in alert_url]\n",
    "\n",
    "# Alert name\n",
    "nn = len(tE_list)\n",
    "alert_name = []\n",
    "for ii in np.arange(nn):\n",
    "    alert_name.append('MB22' + str(ii+1).zfill(3))\n",
    "\n",
    "# Put it all into a dataframe.\n",
    "df = pd.DataFrame(list(zip(alert_name, cat_list, tE_list, Ibase_list, alert_url_list)),\n",
    "                 columns =['alert_name', 'class', 'tE', 'Ibase', 'alert_url'])\n",
    "\n",
    "df.to_sql(con=engine, schema=None, name=\"moa_alerts\", if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b019189",
   "metadata": {},
   "source": [
    "# OGLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6522a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ogle_str_to_float(item):\n",
    "    try:\n",
    "        return float(ne.evaluate(item.contents[0].replace(u'\\n', '')))\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "# Get alerts using beautiful soup.\n",
    "url = \"https://ogle.astrouw.edu.pl/ogle4/ews/2019/ews.html\"\n",
    "response = urlopen(url)\n",
    "html = response.read()\n",
    "response.close()\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "# Grab columns for tE and Ibase.\n",
    "tE = soup.find_all('td')[8::15] \n",
    "Ibase = soup.find_all('td')[13::15]\n",
    "\n",
    "# Convert them from strings to floats.\n",
    "tE_list = [ogle_str_to_float(item) for item in tE]\n",
    "Ibase_list = [ogle_str_to_float(item) for item in Ibase]\n",
    "    \n",
    "# Alert name and page link.\n",
    "nn = len(tE_list)\n",
    "alert_name = []\n",
    "alert_url_list = []\n",
    "ogle_alert_url = 'https://ogle.astrouw.edu.pl/ogle4/ews/'\n",
    "\n",
    "for ii in np.arange(nn):\n",
    "    alert_name.append('OB19' + str(ii+1).zfill(4))\n",
    "    alert_url_list.append(ogle_alert_url + str(ii+1).zfill(4) + '.html')\n",
    "\n",
    "# Put it all into a dataframe.\n",
    "df = pd.DataFrame(list(zip(alert_name, tE_list, Ibase_list, alert_url_list)),\n",
    "                 columns =['alert_name', 'tE', 'Ibase', 'alert_url'])\n",
    "\n",
    "df.to_sql(con=engine, schema=None, name=\"ogle_alerts\", if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2daee3",
   "metadata": {},
   "source": [
    "# KMTNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cd46c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmtnet_str_to_float(item):\n",
    "    try:\n",
    "        return float(ne.evaluate(item.get_text().replace(u'\\xa0', u'')))\n",
    "    except:\n",
    "        return\n",
    "\n",
    "year = '2022'\n",
    "url = \"https://kmtnet.kasi.re.kr/~ulens/event/\" + year + \"/\"\n",
    "response = urlopen(url)\n",
    "html = response.read()\n",
    "response.close()\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "if year in ['2022', '2020', '2017', '2016']:\n",
    "    tE = soup.find_all('td')[7::15][1:]\n",
    "    Ibase = soup.find_all('td')[10::15][1:]\n",
    "    cat = soup.find_all('td')[3::15][1:]\n",
    "elif year in ['2021', '2019', '2018']:\n",
    "    tE = soup.find_all('td')[8::16][1:]\n",
    "    Ibase = soup.find_all('td')[11::16][1:]\n",
    "    cat = soup.find_all('td')[4::16][1:]\n",
    "else:\n",
    "    raise Exception('Not a valid year')\n",
    "\n",
    "tE_list = [kmtnet_str_to_float(item) for item in tE]\n",
    "Ibase_list = [kmtnet_str_to_float(item) for item in Ibase]\n",
    "cat_list = [item.get_text().replace(u'\\xa0', u'') for item in cat]\n",
    "\n",
    "# Link to the alert page.\n",
    "if year in ['2022', '2020', '2017', '2016']:\n",
    "    alert_url = soup.find_all('td')[0::15][1:]\n",
    "elif year in ['2021', '2019', '2018']:\n",
    "    alert_url = soup.find_all('td')[0::16][1:]\n",
    "\n",
    "else:\n",
    "    raise Exception('Not a valid year')\n",
    "\n",
    "kmt_alert_url = 'https://kmtnet.kasi.re.kr/~ulens/event/' + year + '/'\n",
    "alert_url_list = [kmt_alert_url + item.find_all('a', href=True)[0]['href'] for item in alert_url]\n",
    "\n",
    "# Alert name\n",
    "nn = len(tE_list)\n",
    "alert_name = []\n",
    "for ii in np.arange(nn):\n",
    "    alert_name.append('KB22' + str(ii+1).zfill(4))\n",
    "\n",
    "# Put it all into a dataframe.\n",
    "df = pd.DataFrame(list(zip(alert_name, cat_list, tE_list, Ibase_list, alert_url_list)),\n",
    "                 columns =['alert_name', 'class', 'tE', 'Ibase', 'alert_url'])\n",
    "\n",
    "df.to_sql(con=engine, schema=None, name=\"kmt_alerts\", if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-transportation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
